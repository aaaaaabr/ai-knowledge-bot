![MIT License](https://img.shields.io/badge/license-MIT-blue)
![Built with LangChain](https://img.shields.io/badge/Built%20with-LangChain-4b7bec)
![Offline AI](https://img.shields.io/badge/LLM-Ollama-green)
![last commit](https://img.shields.io/github/last-commit/EzioDEVio/ai-knowledge-bot?color=blue)
![repo size](https://img.shields.io/github/repo-size/EzioDEVio/ai-knowledge-bot)
![GitHub issues](https://img.shields.io/github/issues/EzioDEVio/ai-knowledge-bot)
![Forks](https://img.shields.io/github/forks/EzioDEVio/ai-knowledge-bot?style=social)
![Stars](https://img.shields.io/github/stars/EzioDEVio/ai-knowledge-bot?style=social)
![PRs](https://img.shields.io/github/issues-pr/EzioDEVio/ai-knowledge-bot)

# ğŸ§  AI Knowledge Bot

This is my own custom-built offline AI bot that lets you chat with PDFs and web pages using **local embeddings** and **local LLMs** like LLaMA 3.

I built it step by step using LangChain, FAISS, HuggingFace, and Ollama â€” without relying on OpenAI or DeepSeek APIs anymore (they just kept failing or costing too much).

---

## ğŸš€ Features

- ğŸ“„ Chat with uploaded PDF files
- ğŸŒ Ask questions about a webpage URL
- ğŸ§  Uses local HuggingFace embeddings (`all-MiniLM-L6-v2`)
- ğŸ¦™ Powered by Ollama + LLaMA 3 (fully offline LLM)
- ğŸ—ƒï¸ Built-in FAISS vectorstore
- ğŸ§¾ PDF inline preview
- ğŸ§® Built-in calculator + summarizer tools (via LangChain agents)
- ğŸ§  Page citation support (know where each answer came from)
- ğŸ“œ Chat history viewer with download button (JSON)
- ğŸ›ï¸ Simple Streamlit UI with dark/light mode toggle
- ğŸ‘¨â€ğŸ’» Footer credit: *Developed by EzioDEVio*

---

## ğŸ“¦ Tech Stack

- `langchain`, `langchain-community`
- `sentence-transformers` for local embeddings
- `ollama` for local LLMs (`llama3`)
- `PyPDF2` for PDF parsing
- `FAISS` for vector indexing
- `Streamlit` for frontend

---

## ğŸ›  Setup Guide

### 1. Clone this repo

```bash
git clone https://github.com/EzioDEVio/ai-knowledge-bot.git
cd ai-knowledge-bot
````

---

### 2. Create and activate virtualenv (optional but recommended)

```bash
python -m venv venv
.\venv\Scripts\activate  # Windows for Mac is different
```

---

### 3. Install dependencies

```bash
pip install -r requirements.txt
```

Make sure `sentence-transformers` is installed â€” needed for local embeddings.

---

### 4. Install Ollama (for local LLM)

Download and install from:

ğŸ‘‰ [https://ollama.com/download](https://ollama.com/download)

After installation, verify:

```bash
ollama --version
```

Then pull and run the model:

```bash
ollama run llama3
```

> This will download the LLaMA 3 model (approx. 4â€“8GB). You can also try `mistral`, `codellama`, etc.

---

### 5. Run the app

```bash
streamlit run app.py
```

The app will open at:

```
http://localhost:8501
```

---

## ğŸ“ Folder Structure

```
ai-knowledge-bot/
â”œâ”€â”€ app.py                     # Main Streamlit UI
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ pdf_loader.py          # PDF text extraction
â”‚   â”œâ”€â”€ web_loader.py          # Webpage scraper
â”‚   â”œâ”€â”€ vector_store.py        # Embedding + FAISS
â”‚   â””â”€â”€ qa_chain.py            # LLM QA logic (Ollama + tools)
â”œâ”€â”€ .env                       # Not used anymore (was for API keys)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âœ… Working Setup Summary

| Component        | Mode                                 |
| ---------------- | ------------------------------------ |
| Embeddings       | Local (`HuggingFace`)                |
| Vectorstore      | Local (`FAISS`)                      |
| LLM Response     | Local (`Ollama` + `llama3`)          |
| Internet Needed? | âŒ Only for first-time model download |

---

## âš ï¸ Why I Avoided OpenAI / DeepSeek

* **OpenAI** failed with `RateLimitError` and quota issues unless I added billing.
* **DeepSeek** embedding endpoints didnâ€™t work â€” only chat models supported.

So I switched to:

* ğŸ” Local `HuggingFaceEmbeddings` for vectorization
* ğŸ¦™ `ChatOllama` for full offline AI answers

---

## âœ… Now Completed Features

* âœ… PDF upload + preview
* âœ… URL content QA
* âœ… Chat history with page citations
* âœ… Calculator + summarizer tools
* âœ… Footer attribution
* âœ… JSON export
* âœ… 100% offline functionality


---

## ğŸ³ Run with Docker (Secure Production Mode)

Build and run the app securely using a **multi-stage Dockerfile**:

 1. Build the container

```bash
docker build -t ai-knowledge-bot .
```


2. Run the container
Make sure Ollama is running on the host, open up a powershell or in different terminal then:

docker run -p 8501:8501 \
  --add-host=host.docker.internal:host-gateway \
  ai-knowledge-bot
---
## ğŸ” Dockerfile Security Highlights
âœ… Multi-stage build (separates dependencies from runtime)

âœ… Minimal base (python:3.10-slim)

âœ… Non-root appuser by default

âœ… .env, venv, logs excluded via .dockerignore

âœ… Exposes only necessary port (8501)

âœ… Automatically starts Streamlit app

---
## ğŸ’¬ License

MIT â€” feel free to fork, use, or improve it.

---

## ğŸ”¥ Built by EzioDEVio | ğŸ‡®ğŸ‡¶ | ğŸ§ 

From concept to offline AI â€” all step by step.

---

